{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":468084,"sourceType":"datasetVersion","datasetId":215616},{"sourceId":9519275,"sourceType":"datasetVersion","datasetId":5795628}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Part 1: Creation of Dataset : pair of fenced and de-fenced images","metadata":{}},{"cell_type":"code","source":"# Importing necesssary libraries to fence the images\nimport os\nimport cv2\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport shutil\nfrom glob import glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Uncomment only to delete the output created pair_dataset\n# shutil.rmtree(\"/kaggle/working/paired_dataset\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define input and output directories\ninput_dir = '/kaggle/input/cocotest2014'\noutput_base_dir = '/kaggle/working/paired_dataset'\nprint(\"Using input directory:\", input_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define subdirectories for train and test splits\ntrain_input_dir  = os.path.join(output_base_dir, 'train', 'input')\ntrain_target_dir = os.path.join(output_base_dir, 'train', 'target')\ntrain_edge_dir   = os.path.join(output_base_dir, 'train', 'edge')\n\ntest_input_dir   = os.path.join(output_base_dir, 'test', 'input')\ntest_target_dir  = os.path.join(output_base_dir, 'test', 'target')\ntest_edge_dir    = os.path.join(output_base_dir, 'test', 'edge')\n\n# Create directories if they don't exist\nfor directory in [train_input_dir, train_target_dir, train_edge_dir, \n                  test_input_dir, test_target_dir, test_edge_dir]:\n    os.makedirs(directory, exist_ok=True)\n\nprint(\"Directories created under:\", output_base_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get all image paths\nimage_paths = glob(os.path.join(input_dir, '*.jpg'))\n\n# Split data (80% train, 20% test)\nsplit_idx = int(0.8 * len(image_paths))\ntrain_paths = image_paths[:split_idx]\ntest_paths = image_paths[split_idx:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Define Processing Function\n\ndef generate_fence(image, grid_size=(80, 40), color=(2,255, 255), thickness=4):\n    fence_image = image.copy()\n    dx, dy = grid_size\n    h, w = fence_image.shape[:2]\n    \n    step_y = dy // 2\n    num_rows = (h // step_y) + 2 \n    \n    for row in range(num_rows):\n        y = row * step_y\n        offset = dx // 2 if row % 2 == 1 else 0\n        num_cols = (w // dx) + 2\n        \n        for col in range(num_cols):\n            x = offset + col * dx\n            top = (x, y - dy // 2)\n            right = (x + dx // 2, y)\n            bottom = (x, y + dy // 2)\n            left = (x - dx // 2, y)\n            \n            pts = np.array([top, right, bottom, left], np.int32)\n            pts = pts.reshape((-1, 1, 2))\n            cv2.polylines(fence_image, [pts], isClosed=True, color=color, thickness=thickness)\n    \n    return fence_image\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_and_save_image(img_path, target_size=(256, 256), out_dirs=None, filename_prefix=''):\n    image = cv2.imread(img_path)\n    if image is None:\n        return\n    image_resized = cv2.resize(image, target_size)\n    image_fenced = generate_fence(image_resized)\n    edge_map = cv2.Canny(image_fenced, threshold1=100, threshold2=200)\n    \n    target_filename = os.path.join(out_dirs['target'], f\"{filename_prefix}_target.png\")\n    input_filename  = os.path.join(out_dirs['input'],  f\"{filename_prefix}_input.png\")\n    edge_filename   = os.path.join(out_dirs['edge'],   f\"{filename_prefix}_edge.png\")\n    \n    cv2.imwrite(target_filename, image_resized)\n    cv2.imwrite(input_filename,  image_fenced)\n    cv2.imwrite(edge_filename,   edge_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Gather and Split the Dataset\nall_image_paths = []\nfor root, dirs, files in os.walk(input_dir):\n    for file in files:\n        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n            all_image_paths.append(os.path.join(root, file))\n            \nprint(f\"Found {len(all_image_paths)} images in the dataset.\")\n\nmax_images = 50000  # Set to None to process all images\nif max_images is not None:\n    all_image_paths = all_image_paths[:max_images]\n\nrandom.shuffle(all_image_paths)\nsplit_idx = int(0.8 * len(all_image_paths))\ntrain_paths = all_image_paths[:split_idx]\ntest_paths  = all_image_paths[split_idx:]\n\nprint(f\"Processing {len(train_paths)} training images and {len(test_paths)} testing images.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process training images.\nprint(\"Processing training images...\")\nfor idx, img_path in enumerate(train_paths):\n    filename_prefix = f\"train_{idx}\"\n    out_dirs = {'input': train_input_dir, 'target': train_target_dir, 'edge': train_edge_dir}\n    process_and_save_image(img_path, target_size=(256, 256), out_dirs=out_dirs, filename_prefix=filename_prefix)\n    if idx % 100 == 0:\n        print(f\"Processed {idx} training images...\")\n\n# Process testing images.\nprint(\"Processing testing images...\")\nfor idx, img_path in enumerate(test_paths):\n    filename_prefix = f\"test_{idx}\"\n    out_dirs = {'input': test_input_dir, 'target': test_target_dir, 'edge': test_edge_dir}\n    process_and_save_image(img_path, target_size=(256, 256), out_dirs=out_dirs, filename_prefix=filename_prefix)\n    if idx % 100 == 0:\n        print(f\"Processed {idx} testing images...\")\n\nprint(\"Processing complete. Images saved under:\", output_base_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show a random image\nrandom_image_path = random.choice(all_image_paths)\nimg = mpimg.imread(random_image_path)\nplt.imshow(img)\nplt.axis('off')  # Hide axes\nplt.title(f\"Random Image: {os.path.basename(random_image_path)}\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part2: De-fence the images","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nfrom torchmetrics.functional import peak_signal_noise_ratio as psnr\nfrom torchmetrics.functional import structural_similarity_index_measure as ssim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Define the Generator (U-Net style)\\\n# We use a UnetSkipConnectionBlock from the pix2pix architecture.\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None, submodule=None,\n                 outermost=False, innermost=False, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=False)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = nn.BatchNorm2d(inner_nc)\n        uprelu   = nn.ReLU(True)\n        upnorm   = nn.BatchNorm2d(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n            down = [downconv]\n            up   = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=False)\n            down = [downrelu, downconv]\n            up   = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=False)\n            down = [downrelu, downconv, downnorm]\n            up   = [uprelu, upconv, upnorm]\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            # Concatenate skip connection\n            return torch.cat([x, self.model(x)], 1)\n\nclass UNetGenerator(nn.Module):\n    def __init__(self, input_nc=4, output_nc=3, num_downs=8, ngf=64):\n        super(UNetGenerator, self).__init__()\n        # Construct unet structure\n        # innermost layer\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, innermost=True)\n        # add intermediate layers with dropout for deeper layers\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, submodule=unet_block, use_dropout=True)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, submodule=unet_block)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, submodule=unet_block)\n        unet_block = UnetSkipConnectionBlock(ngf,     ngf * 2, submodule=unet_block)\n        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# defining the default transforms of the image\ndef default_transform(img, num_channels):\n    # Resize to 256x256, convert to tensor and normalize to [-1, 1]\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5]*num_channels, std=[0.5]*num_channels)\n    ])\n    return transform(img)\n\n# Class to dive deep in de fencing the images\nclass DeFencingDataset(Dataset):\n    def __init__(self, input_dir, target_dir, edge_dir):\n        self.input_paths = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) \n                                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        self.target_paths = sorted([os.path.join(target_dir, f) for f in os.listdir(target_dir)\n                                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n        self.edge_paths = sorted([os.path.join(edge_dir, f) for f in os.listdir(edge_dir)\n                                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n    def __len__(self):\n        return len(self.input_paths)\n    def __getitem__(self, idx):\n        # Load images\n        fenced_img = Image.open(self.input_paths[idx]).convert('RGB')  # 3 channels\n        target_img = Image.open(self.target_paths[idx]).convert('RGB')   # 3 channels\n        edge_img   = Image.open(self.edge_paths[idx]).convert('L')         # 1 channel\n        \n        # Apply transforms (normalization to [-1, 1])\n        fenced_tensor = default_transform(fenced_img, 3)\n        target_tensor = default_transform(target_img, 3)\n        edge_tensor   = default_transform(edge_img, 1)\n        \n        # Concatenate the fenced image and edge map along the channel dimension -> 4 channels\n        input_tensor = torch.cat([fenced_tensor, edge_tensor], dim=0)\n        return input_tensor, target_tensor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Define the Discriminator (PatchGAN)\n\nclass PatchGANDiscriminator(nn.Module):\n    def __init__(self, input_nc=7, ndf=64, n_layers=3):\n        super(PatchGANDiscriminator, self).__init__()\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n                    nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=False),\n                nn.BatchNorm2d(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=False),\n            nn.BatchNorm2d(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, x):\n        return self.model(x)\n\n# The conditional discriminator concatenates the conditioned input (fenced+edge, 4 channels)\n# with the target (or generated de-fenced image, 3 channels) → 7 channels.\nclass DeFencingDiscriminator(nn.Module):\n    def __init__(self):\n        super(DeFencingDiscriminator, self).__init__()\n        self.model = PatchGANDiscriminator(input_nc=7)\n    def forward(self, input_image, target_image):\n        # Concatenate along channel dimension\n        x = torch.cat([input_image, target_image], dim=1)\n        return self.model(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Initialize Models, Losses and Optimizers\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ngenerator = UNetGenerator().to(device)\ndiscriminator = DeFencingDiscriminator().to(device)\n\ncriterion_GAN = nn.BCEWithLogitsLoss()  # Adversarial loss\ncriterion_L1 = nn.L1Loss()              # L1 loss\n\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n\nLAMBDA = 100  # weight for L1 loss\n# Create datasets and dataloaders\ntrain_dataset = DeFencingDataset(train_input_dir, train_target_dir, train_edge_dir)\ntest_dataset  = DeFencingDataset(test_input_dir, test_target_dir, test_edge_dir)\ntrain_loader  = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\ntest_loader   = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Training Loop\nnum_epochs = 10\n\nfor epoch in range(1, num_epochs+1):\n    generator.train()\n    discriminator.train()\n    for i, (input_img, target_img) in enumerate(train_loader):\n        input_img = input_img.to(device)   # shape: [B, 4, 256, 256]\n        target_img = target_img.to(device)   # shape: [B, 3, 256, 256]\n        \n        # ---------------------\n        # Train Generator\n        # ---------------------\n        optimizer_G.zero_grad()\n        fake_img = generator(input_img)      # Generated de-fenced image\n        # Discriminator's output on fake pair\n        pred_fake = discriminator(input_img, fake_img)\n        valid = torch.ones_like(pred_fake, device=device)\n        loss_G_GAN = criterion_GAN(pred_fake, valid)\n        loss_G_L1  = criterion_L1(fake_img, target_img)\n        loss_G = loss_G_GAN + LAMBDA * loss_G_L1\n        loss_G.backward()\n        optimizer_G.step()\n        \n        # ---------------------\n        # Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n        # Real pair loss\n        pred_real = discriminator(input_img, target_img)\n        loss_D_real = criterion_GAN(pred_real, valid)\n        # Fake pair loss (detach fake image)\n        fake_detach = fake_img.detach()\n        pred_fake = discriminator(input_img, fake_detach)\n        fake = torch.zeros_like(pred_fake, device=device)\n        loss_D_fake = criterion_GAN(pred_fake, fake)\n        loss_D = (loss_D_real + loss_D_fake) * 0.5\n        loss_D.backward()\n        optimizer_D.step()\n\n        \n        if i % 50 == 0:\n            print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(train_loader)}] \"\n                  f\"Loss_G: {loss_G.item():.4f} Loss_D: {loss_D.item():.4f}\")\n    if epoch % 10 == 0:\n        generator.eval()\n        with torch.no_grad():\n            for test_input, test_target in test_loader:\n                test_input = test_input.to(device)\n                test_target = test_target.to(device)\n                fake_test = generator(test_input)\n                # Show the first image of the batch\n                def imshow(img, title):\n                    npimg = img.cpu().detach().numpy()\n                    # rescale from [-1,1] to [0,1]\n                    npimg = (npimg + 1) / 2\n                    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n                    plt.title(title)\n                    plt.axis('off')\n                plt.figure(figsize=(15,5))\n                plt.subplot(1,3,1)\n                imshow(test_input[0, :3, :, :], \"Fenced Image\")\n                plt.subplot(1,3,2)\n                imshow(test_target[0], \"Ground Truth\")\n                plt.subplot(1,3,3)\n                imshow(fake_test[0], \"De-fenced (Generated)\")\n                plt.show()\n                break  # only display one batch\n# Save Model Checkpoints\ntorch.save(generator.state_dict(), f\"defencing_generator.pth\")\ntorch.save(discriminator.state_dict(), f\"defencing_discriminator.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Evaluating the model\ndef evaluate_model(generator, test_loader, device):\n    \"\"\"\n    Evaluates the generator on the test dataset and computes the average PSNR and SSIM.\n    \"\"\"\n    generator.eval()\n    total_psnr = 0.0\n    total_ssim = 0.0\n    count = 0\n\n    with torch.no_grad():\n        for input_img, target_img in test_loader:\n            input_img = input_img.to(device)   # shape: [B, 4, 256, 256]\n            target_img = target_img.to(device) # shape: [B, 3, 256, 256]\n            \n            fake_img = generator(input_img)\n            \n            fake_img = (fake_img + 1) / 2.0\n            target_img = (target_img + 1) / 2.0\n            \n            # Compute metrics per image in the batch\n            for i in range(fake_img.size(0)):\n                current_psnr = psnr(fake_img[i], target_img[i], data_range=1.0)\n                # ssim expects a 4D tensor: [B, C, H, W] so we unsqueeze at batch dimension.\n                current_ssim = ssim(fake_img[i].unsqueeze(0), target_img[i].unsqueeze(0), data_range=1.0)\n                total_psnr += current_psnr.item()\n                total_ssim += current_ssim.item()\n                count += 1\n\n    avg_psnr = total_psnr / count\n    avg_ssim = total_ssim / count\n    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n    print(f\"Average SSIM: {avg_ssim:.4f}\")\n    return avg_psnr, avg_ssim\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Visualize the model\ndef visualize_outputs(generator, test_loader, device, num_samples=3):\n    generator.eval()\n    with torch.no_grad():\n        # Get one batch from the test loader\n        for input_img, target_img in test_loader:\n            input_img = input_img.to(device)\n            target_img = target_img.to(device)\n            fake_img = generator(input_img)\n            # Rescale outputs to [0, 1]\n            fake_img = (fake_img + 1) / 2.0\n            target_img = (target_img + 1) / 2.0\n            # For visualization, extract the first 3 channels (RGB) from the input.\n            input_img_vis = (input_img[:, :3, :, :] + 1) / 2.0\n            \n            for i in range(min(num_samples, fake_img.size(0))):\n                plt.figure(figsize=(12,4))\n                plt.subplot(1,3,1)\n                plt.imshow(np.transpose(input_img_vis[i].cpu().numpy(), (1,2,0)))\n                plt.title(\"Fenced Input (RGB)\")\n                plt.axis('off')\n                \n                plt.subplot(1,3,2)\n                plt.imshow(np.transpose(target_img[i].cpu().numpy(), (1,2,0)))\n                plt.title(\"Ground Truth De-fenced\")\n                plt.axis('off')\n                \n                plt.subplot(1,3,3)\n                plt.imshow(np.transpose(fake_img[i].cpu().numpy(), (1,2,0)))\n                plt.title(\"Generated De-fenced\")\n                plt.axis('off')\n                plt.show()\n            break  # Only display one batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. Call the evaluation and visualization function\ngenerator.load_state_dict(torch.load(f\"defencing_generator.pth\"))\ngenerator.to(device)\navg_psnr, avg_ssim = evaluate_model(generator, test_loader, device)\nvisualize_outputs(generator, test_loader, device, num_samples=6)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}